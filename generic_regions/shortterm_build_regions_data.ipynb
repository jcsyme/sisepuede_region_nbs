{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "450d67f8-0bbd-4597-8a94-3dacaa6bc4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add path to sispeuede to sys.path in python\n",
    "import sys\n",
    "import pathlib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "path_git = pathlib.Path(\"/Users/usuario/git\")\n",
    "for subdir in [\n",
    "    \"sisepuede\", \n",
    "    \"sisepuede_data_pipeline\",\n",
    "    \"sisepuede_juypyter\"\n",
    "]:\n",
    "    _PATH_CUR = path_git.joinpath(subdir)\n",
    "    if str(_PATH_CUR) not in sys.path:\n",
    "        sys.path.append(str(_PATH_CUR))\n",
    "\n",
    "path_pipeline = path_git.joinpath(\"sisepuede_data_pipeline\")\n",
    "\n",
    "\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, os.path\n",
    "import pandas as pd\n",
    "import sisepuede.core.attribute_table as att\n",
    "import sisepuede.core.support_classes as sc\n",
    "import sisepuede.legacy.data_api as api\n",
    "import sisepuede.manager.sisepuede_examples as sxl\n",
    "import sisepuede.manager.sisepuede_file_structure as sfs\n",
    "import sisepuede.manager.sisepuede_models as sm\n",
    "import sisepuede.utilities._plotting as spu\n",
    "import sisepuede.utilities._toolbox as sf\n",
    "import sisepuede.visualization.plots as svp\n",
    "\n",
    "\n",
    "import time\n",
    "from typing import *\n",
    "\n",
    "# from sisepuede_data_pipeline\n",
    "import lib.process_utilities as pu\n",
    "import lib.sisepuede_data_constructs as dc\n",
    "import lib._util as lutil\n",
    "\n",
    "# from sisepuede_jupyter\n",
    "import temp_update_fields_from_wv_to_main as temp \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bace3ddf-3451-4aba-afe9-39f02a83773c",
   "metadata": {},
   "source": [
    "# Setup SISEPUEDE elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "139adc29-8253-4620-87fa-7dc55aef5fd4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected IPython. Loading juliacall extension. See https://juliapy.github.io/PythonCall.jl/stable/compat/#IPython\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precompiling NemoMod...\n",
      "Info Given NemoMod was explicitly requested, output will be shown live \u001b[0K\n",
      "\u001b[0KWARNING: Method definition parse_line(String) in module ConfParser at /Users/usuario/.julia/packages/ConfParser/b2fge/src/ConfParser.jl:95 overwritten in module NemoMod at /Users/usuario/.julia/packages/NemoMod/qatsV/src/config_functions.jl:16.\n",
      "\u001b[0KERROR: Method overwriting is not permitted during Module precompilation. Use `__precompile__(false)` to opt-out of precompilation.\n",
      "   1438.6 ms  ? NemoMod\n",
      "[ Info: Precompiling NemoMod [a3c327a0-d2f0-11e8-37fd-d12fd35c3c72] \n",
      "WARNING: Method definition parse_line(String) in module ConfParser at /Users/usuario/.julia/packages/ConfParser/b2fge/src/ConfParser.jl:95 overwritten in module NemoMod at /Users/usuario/.julia/packages/NemoMod/qatsV/src/config_functions.jl:16.\n",
      "ERROR: Method overwriting is not permitted during Module precompilation. Use `__precompile__(false)` to opt-out of precompilation.\n",
      "┌ Info: Skipping precompilation due to precompilable error. Importing NemoMod [a3c327a0-d2f0-11e8-37fd-d12fd35c3c72].\n",
      "└   exception = Error when precompiling module, potentially caused by a __precompile__(false) declaration in the module.\n"
     ]
    }
   ],
   "source": [
    "def get_file_structure(\n",
    "    y0: int = 2015,\n",
    "    y1: int = 2070,\n",
    ") -> Tuple[sfs.SISEPUEDEFileStructure, att.AttributeTable]:\n",
    "    \"\"\"Get the SISEPUEDE File Structure and update the attribute table\n",
    "        with new years.\n",
    "    \"\"\"\n",
    "    # setup some SISEPUEDE variables and update time period\n",
    "    file_struct = sfs.SISEPUEDEFileStructure(\n",
    "        initialize_directories = False,\n",
    "    )\n",
    "\n",
    "    # get some keys\n",
    "    key_time_period = file_struct.model_attributes.dim_time_period\n",
    "    key_year = file_struct.model_attributes.field_dim_year\n",
    "\n",
    "\n",
    "    ##  BUILD THE ATTRIBUTE AND UPDATE\n",
    "\n",
    "    # setup the new attribute table\n",
    "    years = np.arange(y0, y1 + 1, ).astype(int)\n",
    "    attribute_time_period = att.AttributeTable(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                key_time_period: range(len(years)),\n",
    "                key_year: years,\n",
    "            }\n",
    "        ),\n",
    "        key_time_period,\n",
    "        \n",
    "    )\n",
    "\n",
    "    # finally, update the ModelAttributes inside the file structure\n",
    "    (\n",
    "        file_struct\n",
    "        .model_attributes\n",
    "        .update_dimensional_attribute_table(\n",
    "            attribute_time_period,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # return the tuple\n",
    "    out = (file_struct, attribute_time_period, )\n",
    "\n",
    "    return out\n",
    "\n",
    "    \n",
    "# set up some paths\n",
    "_PATH_CUR = pathlib.Path(os.getcwd())\n",
    "_PATH_DATA = _PATH_CUR.joinpath(\"data\")\n",
    "_PATH_OUTPUT = _PATH_DATA.joinpath(\"output\")\n",
    "\n",
    "# export prefix\n",
    "_PREFIX_FILENAME_DATASETBUILD_BY_REGION = \"sisepuede_raw_inputs_latest_\"\n",
    "\n",
    "\n",
    "# model attributes and associated support classes\n",
    "_EXAMPLES = sxl.SISEPUEDEExamples()\n",
    "_FILE_STRUCTURE, _ATTRIBUTE_TABLE_TIME_PERIOD = get_file_structure()\n",
    "matt = _FILE_STRUCTURE.model_attributes\n",
    "regions = sc.Regions(matt, )\n",
    "time_periods = sc.TimePeriods(matt, )\n",
    "\n",
    "# setup models\n",
    "models = sm.SISEPUEDEModels(\n",
    "    matt,\n",
    "    allow_electricity_run = True,\n",
    "    fp_julia = _FILE_STRUCTURE.dir_jl,\n",
    "    fp_nemomod_reference_files = _FILE_STRUCTURE.dir_ref_nemo,\n",
    "    initialize_julia = True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd8518-b25e-4269-8efb-3b998cae75af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "7583d1d4-5fe5-400c-a97c-37b085b7ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  SOME GLOBALS\n",
    "\n",
    "df_example_input = _EXAMPLES(\"input_data_frame\")\n",
    "tab = regions.attributes.table\n",
    "\n",
    "\n",
    "##  NOTE! Need to ignore these for the moment\n",
    "# The GDP for these are incomplete:\n",
    "#   - antigua_and_barbuda\n",
    "#   - cayman_islands\n",
    "# There are other issues with these countries:\n",
    "#   - british_virgin_islands\n",
    "#   - cuba\n",
    "#   - curacao\n",
    "#   - dominica\n",
    "#   - grenada'\n",
    "#   - saint_kitts_and_nevis\n",
    "#   - saint_martin\n",
    "#   - sint_maarten\n",
    "#   - turks_and_caicos_islands\n",
    "#   - united_states_virgin_islands\n",
    "#\n",
    "_REGIONS_TO_CHECK = [\n",
    "    \"antigua_and_barbuda\", \n",
    "    \"british_virgin_islands\",\n",
    "    \"cayman_islands\",\n",
    "    \"cuba\",\n",
    "    \"curacao\",\n",
    "    \"dominica\",\n",
    "    \"grenada\",\n",
    "    \"saint_kitts_and_nevis\",\n",
    "    \"saint_martin\",\n",
    "    \"sint_maarten\",\n",
    "    \"turks_and_caicos_islands\",\n",
    "    \"united_states_virgin_islands\"\n",
    "]\n",
    "\n",
    "_REGIONS_BUILD = list(\n",
    "    tab[\n",
    "        tab[\"un_sub_region\"].isin([\"Latin America and the Caribbean\"])\n",
    "        & ~tab[regions.key].isin(_REGIONS_TO_CHECK)    \n",
    "    ][regions.key]\n",
    "    .unique()\n",
    ")\n",
    "_REGIONS_BUILD = sorted(\n",
    "    _REGIONS_BUILD +\n",
    "    [\n",
    "        \"bulgaria\",\n",
    "        \"egypt\",\n",
    "        \"libya\",\n",
    "        \"morocco\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "_REGIONS_BUILD = [\n",
    "    \"china\",\n",
    "    \"united_states_of_america\"\n",
    "]\n",
    "\n",
    "_REGIONS_ISO = [\n",
    "    regions.return_region_or_iso(x, return_type = \"iso\", )\n",
    "    for x in _REGIONS_BUILD\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8acb21c-858f-4817-ad81-3f6590ce9804",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff35ceaf-a5de-488d-9086-ef56266619d9",
   "metadata": {},
   "source": [
    "# Setup old repository and read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43902ad2-149e-46b6-9220-e1c96328623e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting old repository data...\n",
      "Old repository data complete.\n"
     ]
    }
   ],
   "source": [
    "repo_old = api.SISEPUEDEBatchDataRepository(\n",
    "    \"/Users/usuario/git/sisepuede_data\", \n",
    "    matt,\n",
    ")\n",
    "\n",
    "print(\"Getting old repository data...\")\n",
    "df_old = repo_old.read(None)\n",
    "print(\"Old repository data complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a81d4d-3c34-4434-b2e1-f7f27624d9ac",
   "metadata": {},
   "source": [
    "# Setup new repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "ee0a544d-ced6-4adc-8d01-6e7aae52df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dc)\n",
    "construct = dc.SISEPUEDEDataConstructs(\n",
    "    path_output_database = \"/Users/usuario/git/sisepuede_data_pipeline/sisepuede_inputs.sqlite\",\n",
    ")\n",
    "\n",
    "path_repo = pathlib.Path(\"/Users/usuario/SISEPUEDE_DATA_REPOSITORY\")\n",
    "repo = pu.Repository(\n",
    "    {\n",
    "        \"local\": {\n",
    "            \"path\": str(path_repo)\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# get from pipeline\n",
    "df_from_pipeline = construct.build_inputs_from_database(\n",
    "    regions_keep = _REGIONS_BUILD,\n",
    "    join = \"outer\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "4b0e4558-db05-40cb-b6fc-a78d177ce0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def function_combine(\n",
    "    df_repo_new: pd.DataFrame,\n",
    "    df_repo_old: pd.DataFrame,\n",
    "    df_example: pd.DataFrame,\n",
    "    region_iso: str,\n",
    "    years: Union[List[int], None] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Combine DataFrames by region, combining in a hierarchy\n",
    "    \"\"\"\n",
    "    \n",
    "    ##  FORMAT A BASE \n",
    "\n",
    "    # start by setting years\n",
    "    if not sf.islistlike(years):\n",
    "        years = time_periods.all_years\n",
    "\n",
    "    \n",
    "    df_base = (\n",
    "        df_repo_old[\n",
    "            df_repo_old[repo_old.field_repo_iso].isin([region_iso])\n",
    "            & (df_repo_old[time_periods.field_year] >= min(years))\n",
    "        ]\n",
    "        .copy()\n",
    "        .rename(\n",
    "            columns = {\n",
    "                repo_old.field_repo_iso: regions.field_iso,\n",
    "                repo_old.field_repo_year: time_periods.field_year,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # cols_drop = [\n",
    "    #     x for x in df_base.columns \n",
    "    #     if df_base[x].dropna().shape[0] != df_base.shape[0]\n",
    "    # ]\n",
    "    #\n",
    "    # df_base = df_base.drop(columns = cols_drop, )\n",
    "    df_base[time_periods.field_year] = df_base[time_periods.field_year].astype(int)    \n",
    "    df_base = (\n",
    "        pd.merge(\n",
    "            pd.DataFrame({time_periods.field_year: years, }),\n",
    "            df_base,\n",
    "            how = \"left\",\n",
    "        )\n",
    "        .ffill()\n",
    "        .bfill()\n",
    "    )\n",
    "        \n",
    "\n",
    "    ##  ADD IN PIPELINE DATA\n",
    "\n",
    "    df_out = sf.match_df_to_target_df(\n",
    "        df_base, \n",
    "        df_from_pipeline,\n",
    "        [\n",
    "            construct.time_periods.field_year,\n",
    "            regions.field_iso,\n",
    "        ],\n",
    "        overwrite_only = False,\n",
    "    )\n",
    "    \n",
    "    df_out[time_periods.field_year] = df_out[time_periods.field_year].astype(int)\n",
    "    df_out = (time_periods.years_to_tps(df_out, ))\n",
    "\n",
    "\n",
    "    ##  PULL MISSING FIELDS FROM EXAMPLE DF\n",
    "    \n",
    "    # fields not in peru\n",
    "    fields_missing = [\n",
    "        x for x in df_example.columns \n",
    "        if (x not in df_out.columns) \n",
    "        and (x in matt.all_variable_fields_input)\n",
    "    ]\n",
    "    \n",
    "    # specify fields to pull from the example\n",
    "    fields_from_ex = [\n",
    "        x for x in fields_missing \n",
    "        if not (\n",
    "            False#x.startswith(\"frac_lndu_\")\n",
    "            #x.startswith(\"factor_lndu\")\n",
    "            #or x.startswith(\"frac_lndu_\")\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    fields_from_ex = [\n",
    "        x for x in fields_from_ex\n",
    "        if (x not in df_out.columns)\n",
    "    ]\n",
    "\n",
    "    # merge in from ex\n",
    "    df_out = (\n",
    "        pd.merge(\n",
    "            df_out,\n",
    "            df_example[fields_from_ex + [time_periods.field_time_period]],\n",
    "            how = \"left\"\n",
    "        )\n",
    "        .ffill()\n",
    "        .bfill()\n",
    "    )\n",
    "\n",
    "\n",
    "    ##  TEMPORARY SCRIPT FOR MOVING FROM working_version TO latest full version\n",
    "    \n",
    "    df_out = temp.update_fields(\n",
    "        df_out,\n",
    "        matt,\n",
    "    )\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "df_abw = function_combine(\n",
    "    df_from_pipeline,\n",
    "    df_old,\n",
    "    df_example_input,\n",
    "    \"ARG\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e5372-176d-4d33-b0cd-89c049f4f665",
   "metadata": {},
   "source": [
    "# Run the next cell to build a composite file and/or export individual fiels (set `export = True` to do this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "418ed9a5-86b0-4bed-b0bc-e8bd762b9829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHN\n",
      "USA\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_out = []\n",
    "export = True\n",
    "# years = range(2015, 2071)\n",
    "\n",
    "def build_path_for_region_file(\n",
    "    iso: str,\n",
    ") -> pathlib.Path:\n",
    "    \"\"\"Build the output path for a file\n",
    "    \"\"\"\n",
    "    fn = f\"{_PREFIX_FILENAME_DATASETBUILD_BY_REGION}{iso}.csv\"\n",
    "    path_out = _PATH_OUTPUT.joinpath(fn)\n",
    "    \n",
    "    return path_out\n",
    "\n",
    "\n",
    "\n",
    "for iso in _REGIONS_ISO:\n",
    "    print(iso)\n",
    "    df_cur = function_combine(\n",
    "        df_from_pipeline,\n",
    "        df_old,\n",
    "        df_example_input,\n",
    "        iso,\n",
    "    )    \n",
    "    df_cur[regions.field_iso] = iso\n",
    "\n",
    "    # write to the output location?\n",
    "    if export:\n",
    "        path_write = build_path_for_region_file(iso, )\n",
    "        df_cur.to_csv(\n",
    "            path_write,\n",
    "            encoding = \"UTF-8\",\n",
    "            index = None,\n",
    "        )\n",
    "    \n",
    "    df_out.append(df_cur,)\n",
    "\n",
    "df_out = sf._concat_df(df_out, )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea534057-5bec-494f-bfd5-e35c74692fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f46168ab-1218-4ea3-bff0-89196e56bec1",
   "metadata": {},
   "source": [
    "# option to test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b2a914-fa37-4e23-b1b1-891f1d47b058",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_run = \"CHN\"\n",
    "models.project(\n",
    "    df_out[\n",
    "        df_out[regions.field_iso].isin([region_run])\n",
    "    ].reset_index(drop = True),\n",
    "    time_periods_base = np.arange(12),\n",
    "    verbose = True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b37357-c91a-4241-985f-8443fb23e920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d41719-b678-43c0-a8d9-2b6f0f67f120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28979b67-8f3a-40ab-837b-748dd6676742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e679a25-31c4-4c21-898b-dac003929333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794499cb-3e1e-4f38-96a3-0edf0546f707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
